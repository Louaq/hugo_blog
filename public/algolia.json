
[
  
  
  {
    "objectID": "1735478641",
    "permalink": "/post/visual-studio-code-and-latex/",
    "title": "vscode配置latex环境",
    
    "content": " 前言 LaTeX 作为一种强大的排版系统，对于理工科，特别是公式比较多的数学专业（秃头专业），其重要性自不必多说，不在本文探讨范围之内。\n而选择一个比较好的编译器是很重要的，至少对笔者而言是如此。笔者前期使用的是TeXstudio进行文档的编译的，但是其编译速度比较慢，并且页面不是很美观。最让人头疼的是，当公式比较长的时候，使用的括号就比较多，但Texstudio的代码高亮功能实在是…（它对于括号根本就没有高亮，头秃）\n而Visual Studio Code呢？话不多说，直接上图！\n可以看到，它不仅能够对代码高亮，不同级别括号用不同颜色标注了，颜值也很高。且 vscode 最突出的特点就是其强大的插件功能，每个使用者都能够根据自己的需求和想法下载相应的插件，从而将之配置为高度个性化的编辑器。可以这么说，每个使用者的 vscode 都不一样，为其专属定制编辑器。\n笔者配置了好久，找了很多资料，很多博主也只是贴上了配置代码，没有详细的介绍说明。为了让更多人能够有一个比较清晰的了解，以此可以随时对自己的配置代码进行更改，故笔者写下了此文。希望能够对大家有所帮助。\n注 1： 本文使用图片均为笔者自身编辑器截图或笔者朋友的编辑器截图（经过对方同意），且所有引用在文中或文末注明了来源，其余均为原创内容（代码不算哈哈哈）。\n注 2： 若您的 vscode 页面和笔者所用图片中展示的页面有略微不同，均为笔者所安装的其余插件以及其余设置所致，并不影响本文中所说的所有配置，您无需担心，只需按照图片中所指向图标进行配置即可。\n注 3： 文末有完整的个人配置代码（有的地方需要更改路径，有具体说明）。\n1 TeX Live 下载与安装 笔者选用的 Tex 系统是 TeX Live ，如果您想了解 TeX Live 和 MiKTeX 的区别，可以查看此篇文章：https://www.cnblogs.com/liuliang1999/p/12656706.html\n接下来是 TeX Live 的下载与安装说明：\n① 通过网址 ：http://tug.org/texlive/acquire-iso.html 进入 ISO 下载页面，点击图示红框圈画位置进入随机的镜像网站。\n② 可以看到的是，笔者进入了清华大学镜像网站，点击红框圈画链接进行 TeX Live 下载。\n③ 如果下载速度过慢，可以返回 …",
    
    "date": "2024-12-29 13:24:01",
    "updated": "2024-12-29 13:24:01"
  }
  
  , 
  {
    "objectID": "1732536000",
    "permalink": "/post/article_1/",
    "title": "YOLOv8的训练自己的数据集",
    
    "content": " 一、YOLOv8的简介 YOLO（You Only Look Once）系列算法因其高效、准确等特点而备受瞩目。由2023年Ultralytics公司发布了YOLO的最新版本YOLOv8是结合前几代YOLO的基础上的一个融合改进版。\n本文YOLOv8网络结构/环境搭建/数据集获取/训练/推理/验证/导出/部署，从网络结构的讲解从模型的网络结构讲解到模型的部署都有详细介绍，同时在本专栏中还包括YOLOv8模型系列的改进包括个人提出的创新点，传统卷积、注意力机制、损失函数的修改教程，能够帮助你的论文获得创新点。\n二、YOLOv8相对于Yolov5的核心改动 从YOLOv8的网络结构可以看出,其延用了YOLOv5的网络结构思想，包括基于CSP（紧凑和分离）的骨干网络(backbone)和Neck部分的设计，以及对于不同尺度模型的考虑。\n改进总结：\nBackbone的改进：使用C2f模块代替C3模块，进一步轻量化，同时保持了CSP的思想，同时采用了SPPF模块。\nPAN-FPN的改进：保留了PAN的思想，但删除了上采样阶段中的卷积结构，同时将C3模块替换为C2f模块。\nDecoupled-Head的引入：采用了Decoupled-Head的思想，使得网络的训练和推理更加高效。\nAnchor-Free的思想：抛弃了Anchor-Base，采用了Anchor-Free的思想。\n损失函数的改进：采用VFL Loss作为分类损失，同时使用DFL Loss和CIOU Loss作为回归损失。\n样本匹配方式的改进：采用了Task-Aligned Assigner匹配方式。\n这些改进使得YOLOv8在目标检测方面具有更高的精度和更快的速度，同时保持了轻量化的特点。\n具体来说，YOLOv8的Backbone部分使用了C2f模块来替代了YOLOv5中的C3模块，实现了进一步的轻量化。同时，它也保留了YOLOv5等架构中使用的SPPF（空间金字塔池化）模块。\n在PAN-FPN（路径聚合网络-特征金字塔网络）部分，虽然YOLOv8依旧采用了PAN的思想，但是在结构上，它删除了YOLOv5中PAN-FPN上采样阶段中的卷积结构，并将C3模块替换为了C2f模块。\n这些改进使得YOLOv8在保持了YOLOv5网络结构的优点的同时，进行了更加精细的调整和优化，提高了模型在不同场景下的性能。\n三、YOLOv8的网络结构 YOLOv8的网络结构主要由以下三个大部分组成：\nBackbone：它采用了一系列卷积和反卷积层来提取特征，同时也使用了残差连接和瓶颈结构来减小网络的大小和提高性能。该部分采用了C2f模块作为基本构成单元，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。\nNeck：它采用了多尺度特征融合技术，将来自Backbone的不同阶段的特征图进行融合，以增强特征表示能力。具体来说，YOLOv8的Neck部分包括一个SPPF模块、一个PAA模块和两个PAN模块。\nHead：它负责最终的目标检测和分类任务，包括一个检测头和一个分类头。检测头包含一系列卷积层和反卷积层，用于生成检测结果；分类头则采用全局平均池化来对每个特征图进行分类。\n下面我们来针对于YOLOv8的三个组成部分进行详细讲解。\n3.1 Backbone 由最上面的YOLOv8网络结构图我们可以看出在其中的Backbone部分，由5个卷积模块和4个C2f模块和一个SPPF模块组成，\n(其中浅蓝色为卷积模块,黄色为C2f模块深蓝色为SPPF模块 )\n如果上图看的不够直观,我们来看一下YOLOv8的文件中的yaml文件,看一下它backbone部分的结构组成部分，会更加直观。 backbone: # [from, repeats, module, args] - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2 - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4 - [-1, 3, C2f, [128, True]] - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8 - [-1, 6, C2f, [256, True]] - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16 - [-1, 6, C2f, [512, True]] - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32 - [-1, 3, C2f, [1024, True]] - [-1, 1, SPPF, [1024, 5]] # 9 上面的部分就是YOLOv8的yaml文件的Backbone部分，可以看到其由5个Conv模块，四个C2f模块以及一个SPPF模块组成，下面我们来根据每个模块的组成来进行讲解。\n3.1.1 卷积模块(Conv) 在其中卷积模块的结构主要为下图\n在其中主要结构为一个2D的卷积一个BatchNorm2d和一个SiLU激活函数，整个卷积模块的作用为：\n降采样：每个卷积模块中的卷积层都采用步长为2的卷积核进行降采样操作，以减小特征图的尺寸并增加通道数。 非线性表示：每个卷积层之后都添加了Batch Normalization（批标准化）层和ReLU激活函数，以增强模型的非线性表示能力。 在其中Batch Normalization（批标准化）是深度学习中常用的一种技术，用于加速神经网络的训练。Batch Normalization通过对每个小批量数据进行标准化，使得神经网络在训练过程中更加稳定，可以使用更高的学习率，并且减少了对初始化权重的依赖。Batch Normalization的基本思想是：对每个小批量数据进行标准化，使得每个特征的均值为0，方差为1，然后再通过一个可学习的缩放因子和平移因子来调整数据的分布，从而使得神经网络更容易训练。\n3.1.2 C2f模块 在YOLOv8的网络结构中C2f模块算是YOLOv8的一个较大的改变，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。下图为C2f的内部网络结构图。\n在C2f模块中我们可以看到输入首先经过一个k=1，s=1，p=0，c=c_out的卷积模块进行了处理，然后经过一个split处理**(在这里split和后面的concat的组成其实就是所谓的残差模块处理)**经过数量为n的DarknetBottleneck模块处理以后将残差模块和主干模块的结果进行Concat拼接在经过一个卷积模块处理进行输出。 在其中提到的残差连接（residual connections）是一种用于构建深层神经网络的技术。它的核心思想是通过跳过层级连接来传递残差或误差。\n在传统的神经网络中，信息流通过一层层的网络层，每一层都通过非线性激活函数进行转换和提取特征。然而，随着神经网络的加深，可能会出现\u0026quot;梯度消失\u0026quot;或\u0026quot;梯度爆炸\u0026quot;的问题，导致网络收敛困难或性能下降。\n残差连接通过引入跨层级的连接，将输入的原始信息直接传递到后续层级，以解决梯度消失和爆炸问题。具体而言，它将网络的输入与中间层的输出相加，形成了一个\u0026quot;捷径\u0026quot;或\u0026quot;跳跃连接\u0026quot;，从而允许梯度更容易地传播。\n数学上，假设我们有一个输入x，通过多个网络层进行处理后得到预测值H(x)。那么残差连接的表达式为：\nF(x) = H(x) + x\n其中，F(x)为残差块的输出，H(x)为经过一系列网络层处理后得到的特征表示，x为输入直接连接到残差块中的跳跃连接。\n通过残差连接，网络可以更容易地学习残差或误差，从而使网络更深层次的特征表达更准确。这对于训练深层神经网络非常有用，可以提高网络的性能和收敛速度。\n在C2f模块中用到的DarknetBottleneck模块其中使用多个3x3卷积核进行卷积操作，提取特征信息。同时其具有add是否进行残差链接的选项。\n其实整个C2f模块就是一个改良版本的Darknet\n首先，使用1x1卷积核将输入通道数减少到原来的1/2，以减少计算量和内存消耗。\n然后，使用多个3x3卷积核进行卷积操作，提取特征信息。\n接着，使用残差链接，将输入直接加到输出中，从而形成了一条跨层连接。\n接着，再次使用1x1卷积核恢复特征图的通道数。\nSPPF模块 YOLOv8的SPPF模块相对于YOLOv5的SPPF模块并没有任何的改变。\n3.2 Neck YOLOv8的Neck部分是该模型中的一个关键组件，它在特征提取和融合方面起着重要作用。Neck的详细描述如下：\nNeck部分主要起到一个特征融合的操作, YOLOv8的Neck部分依然采用PAN-FPN的思想，下图的a，b，c为一个Neck部分的流程示意图。\n整个Neck部分的步骤如下：：将特征提取网络(Backbone)的输出P3，P4，P5输入进PAN-FPN网络结构，使得多个尺度的特征图进行融合；将P5经过上采样与P4进行融合得到F1，将F1经过C2f层和一次上采样与P3进行融合得到T1，将T1经过一次卷积层与F1经过融合得到F2，将F2经过一次C2f层得到T2，将T2经过一次卷积层与P5融合得到F3，将F3经过一次C2f层得到T3，最终得到T1、T2、T3就是整个Neck的产物； 上述过程可以描述为下图，我在图片上做了一些标准方便理解。\n上述的过程可以在代码部分看到,我们同样看YOLOv8的yaml文件，能够更直观的看到这个步骤,大家可以看代码同时对应图片来进行分析:\nhead: - [-1, 1, nn.Upsample, [None, 2, \u0026#39;nearest\u0026#39;]] - [[-1, 6], 1, Concat, [1]] # cat backbone P4 - [-1, 3, C2f, [512]] # 12 - [-1, 1, nn.Upsample, [None, 2, \u0026#39;nearest\u0026#39;]] - [[-1, 4], 1, Concat, [1]] # cat backbone P3 - [-1, 3, C2f, [256]] # 15 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 12], 1, Concat, [1]] # cat head P4 - [-1, 3, C2f, [512]] # 18 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 9], 1, Concat, [1]] # cat head P5 - [-1, 3, C2f, [1024]] # 21 (P5/32-large) Neck部分的整体功能的详细分析如下:\n1. Neck的作用：\nNeck部分在YOLOv8模型中负责对来自Backbone的特征进行进一步处理和融合，以提高目标检测的准确性和鲁棒性。它通过引入不同的结构和技术，将多尺度的特征图进行融合，以便更好地捕捉不同尺度目标的信息。\n2. 特征金字塔网络（Feature Pyramid Network, FPN）：\nYOLOv8的Neck部分通常采用特征金字塔网络结构，用于处理来自Backbone的多尺度特征图。FPN通过在不同层级上建立特征金字塔，使得模型能够在不同尺度上进行目标检测。它通过上采样和下采样操作，将低层级的细节特征与高层级的语义特征进行融合，以获取更全面和丰富的特征表示。\n3. 特征融合（Feature Fusion）：\nNeck部分还包括特征融合的操作，用于将来自不同层级的特征进行融合。这种特征融合有助于提高模型对目标的检测准确性，尤其是对于不同尺度的目标。\n4. 上采样和下采样：\nNeck部分通常会使用上采样和下采样操作，以调整特征图的尺度和分辨率。上采样操作可以将低分辨率的特征图放大到与高分辨率特征图相同的尺寸，以保留更多的细节信息。而下采样操作则可以将高分辨率的特征图降低尺寸，以减少计算量和内存消耗。\nYOLOv8的Neck部分通过特征金字塔网络和特征融合等操作，有效地提取和融合多尺度的特征，从而提高了目标检测的性能和鲁棒性。这使得模型能够更好地适应不同尺度和大小的目标，并在复杂场景下取得更准确的检测结果。\nPAN-FPN（具有特征金字塔网络的路径聚合网络）是一种用于计算机视觉中对象检测的神经网络架构。它将特征金字塔网络（FPN）与路径聚合网络（PAN）相结合，以提高目标检测的准确性和效率。\nFPN 用于从不同比例的图像中提取特征，而 PAN 用于跨网络的不同层聚合这些特征。这允许网络检测不同大小和分辨率的对象，并处理具有多个对象的复杂场景。\n3.3 Head 如果Backbone和Neck部分可以理解为准备工作，那么Head部分就是收获的部分，经过前面的准备工作我们得到了Neck部分的输出T1、T2、T3分别代表不同层级的特征图，Head部分就是对这三个特征图进行处理以产生模型的的输出结果的一个过程。\nYOLOv8的Head部分我们先来看一下它的网络结构。\n可以看到在YOLOv8的Head部分，体现了最核心的改动——\u0026gt;解耦头(Decoupled-Head)，顾名思义就是将原先的一个检测头分解成两个部分。\n在Head部分的三个解耦头分别对应着Neck部分的特征图输出T1、T2、T3。、\n解耦头的工作流程是：\n将网络得到的特征图T1，T2，T3分别输入解耦头头进行预测，检测头的结构如下图所示其中包含4个3×3卷积与2个1×1卷积，同时在检测头的回归分支中添加WIOU损失函数如图4所示，回归头部需要计算预测框与真实框之间的位置偏移量，然后将偏移量送入回归头部进行损失计算，然后输出一个四维向量，分别表示目标框的左上角坐标x、y和右下角坐标x、y。分类头部针对于每个Anchor Free提取的候选框对其进行RoI Pooling和卷积操作得到一个分类器输出张量每个位置上的值表示该候选框属于每个类别的概率，在最后通过极大值抑制方式筛选出最终的检测结果 我们再从YOLOv8的yaml文件来看Head部分的作用\n可以看到检测头部分的输出为15,18，21分别对应着Neck部分的三个输出特征图。 到此YOLOv8的网络结构部分讲解就已经完成，下面我们来看如何利用YOLOv8进行训练操作。 四、环境搭建 在我们配置好环境之后，在之后模型获取完成之后，我们可以进行配置的安装我们可以在命令行下输入如下命令进行环境的配置。\npip install -r requirements.txt 输入如上命令之后我们就可以看到命令行在安装模型所需的库了。 五、数据集获取 我在上面随便下载了一个 数据集用它导出yolov8的数据集，以及自动给转换成txt的格式yaml文件也已经配置好了，我们直接用就可以。 六、模型获取 到这里假设你已经搭建好了环境和有了数据集，那么我们就可以进行模型的下载，因为yolov8目前还存在BUG并不稳定随时都有可能进行更新，所以不推荐大家通过其它的途径下载，最好通过下面的方式进行下载。\n我们可以直接在终端命令下\n(PS：这里需要注意的是我们需要在你总项目文件目录下输入这个命令，因为他会下载到当前目录下)\npip install ultralytics 如果大家去github上直接下载zip文件到本地可能会遇到报错如下，识别不了yolo命令，所以推荐大家用这种方式下载，\n七、模型训练 我们来看一下主要的ultralytics目录结构，\n我门打开cfg目录下的default.yaml文件可以配置模型的参数，\n在其中和模型训练有关的参数及其解释如下:\n参数名 输入类型 参数解释 0 task str YOLO模型的任务选择，选择你是要进行检测、分类等操作 1 mode str YOLO模式的选择，选择要进行训练、推理、输出、验证等操作 2 model str/optional 模型的文件，可以是官方的预训练模型，也可以是训练自己模型的yaml文件 3 data str/optional 模型的地址，可以是文件的地址，也可以是配置好地址的yaml文件 4 epochs int 训练的轮次，将你的数据输入到模型里进行训练的次数 5 patience int 早停机制，当你的模型精度没有改进了就提前停止训练 6 batch int 我们输入的数据集会分解为多个子集，一次向模型里输入多少个子集 7 imgsz int/list 输入的图片的大小，可以是整数就代表图片尺寸为int*int，或者list分别代表宽和高\\[w，h\\] 8 save bool 是否保存模型以及预测结果 9 save_period int 在训练过程中多少次保存一次模型文件,就是生成的pt文件 10 cache bool 参数cache用于控制是否启用缓存机制。 11 device int/str/list/optional GPU设备的选择：cuda device=0 or device=0,1,2,3 or device=cpu 12 workers int 工作的线程，Windows系统一定要设置为0否则很可能会引起线程报错 13 name str/optional 模型保存的名字，结果会保存到\u0026rsquo;project/name\u0026rsquo; 目录下 14 exist_ok bool 如果模型存在的时候是否进行覆盖操作 15 prepetrained 7.1 训练的三种方式 7.1.1 方式一 我们可以通过命令直接进行训练在其中指定参数，但是这样的方式，我们每个参数都要在其中打出来。命令如下:\nyolo task=detect mode=train model=yolov8n.pt data=data.yaml batch=16 epochs=100 imgsz=640 workers=0 device=0 需要注意的是如果你是Windows系统的电脑其中的Workers最好设置成0否则容易报线程的错误。\n7.1.2 方式二（推荐） 通过指定cfg直接进行训练，我们配置好ultralytics/cfg/default.yaml这个文件之后，可以直接执行这个文件进行训练，这样就不用在命令行输入其它的参数了。\nyolo cfg=ultralytics/cfg/default.yaml 7.1.3 方式三 我们可以通过创建py文件来进行训练，这样的好处就是不用在终端上打命令，这也能省去一些工作量，我们在根目录下创建一个名字为run.py的文件，在其中输入代码\nfrom ultralytics import YOLO model = YOLO(\u0026#34;权重的地址\u0026#34;) data = \u0026#34;文件的地址\u0026#34; model.train(data=data, epochs=100, batch=16) 无论通过上述的哪一种方式在控制台输出如下图片的内容就代表着开始训练成功了！\n八、模型验证/测试 参数名 类型 参数讲解 1 val bool 用于控制是否在训练过程中进行验证/测试。 2 split str 用于指定用于验证/测试的数据集划分。可以选择 \u0026lsquo;val\u0026rsquo;、\u0026rsquo;test\u0026rsquo; 或 \u0026rsquo;train\u0026rsquo; 中的一个作为验证/测试数据集 3 save_json bool 用于控制是否将结果保存为 JSON 文件 4 save_hybird bool 用于控制是否保存标签和附加预测结果的混合版本 5 conf float/optional 用于设置检测时的目标置信度阈值 6 iou float 用于设置非极大值抑制（NMS）的交并比（IoU）阈值。 7 max_det int 用于设置每张图像的最大检测数。 8 half bool 用于控制是否使用半精度（FP16）进行推断。 9 dnn bool ，用于控制是否使用 OpenCV DNN 进行 ONNX 推断。 10 plots bool 用于控制在训练/验证过程中是否保存绘图结果。 验证我们划分的验证集/测试集的情况，也就是评估我们训练出来的best.pt模型好与坏\nyolo task=detect mode=val model=best.pt data=data.yaml device=0 九、模型推理 我们训练好自己的模型之后，都会生成一个模型文件,保存在你设置的目录下,当我们再次想要实验该模型的效果之后就可以调用该模型进行推理了，我们也可以用官方的预训练权重来进行推理。\n推理的方式和训练一样我们这里就选一种来进行举例其它的两种方式都是一样的操作只是需要改一下其中的一些参数即可:\n参数讲解\n参数名 类型 参数讲解 0 source str/optinal 用于指定图像或视频的目录 1 show bool 用于控制是否在可能的情况下显示结果 2 save_txt bool 用于控制是否将结果保存为 .txt 文件 3 save_conf bool 用于控制是否在保存结果时包含置信度分数 4 save_crop bool 用于控制是否将带有结果的裁剪图像保存下来 5 show_labels bool 用于控制在绘图结果中是否显示目标标签 6 show_conf bool 用于控制在绘图结果中是否显示目标置信度分数 7 vid_stride int/optional 用于设置视频的帧率步长 8 stream_buffer bool 用于控制是否缓冲所有流式帧（True）或返回最新的帧（False） 9 line_width int/list\\[int\\]/optional 用于设置边界框的线宽度，如果缺失则自动设置 10 visualize bool 用于控制是否可视化模型的特征 11 augment bool 用于控制是否对预测源应用图像增强 12 agnostic_nms bool 用于控制是否使用无关类别的非极大值抑制（NMS） 13 classes int/list\\[int\\]/optional 用于按类别筛选结果 14 retina_masks bool 用于控制是否使用高分辨率分割掩码 15 boxes bool 用于控制是否在分割预测中显示边界框。 yolo task=detect mode=predict model=best.pt source=images device=0 这里需要需要注意的是我们用模型进行推理的时候可以选择照片也可以选择一个视频的格式都可以。支持的视频格式有 MP4（.mp4）：这是一种常见的视频文件格式，通常具有较高的压缩率和良好的视频质量\nAVI（.avi）：这是一种较旧但仍广泛使用的视频文件格式。它通常具有较大的文件大小\nMOV（.mov）：这是一种常见的视频文件格式，通常与苹果设备和QuickTime播放器相关\nMKV（.mkv）：这是一种开放的多媒体容器格式，可以容纳多个视频、音频和字幕轨道\nFLV（.flv）：这是一种用于在线视频传输的流式视频文件格式\n十、模型输出 当我们进行部署的时候可以进行文件导出，然后在进行部署。\nYOLOv8支持的输出格式有如下\n1. ONNX（Open Neural Network Exchange）：ONNX 是一个开放的深度学习模型表示和转换的标准。它允许在不同的深度学习框架之间共享模型，并支持跨平台部署。导出为 ONNX 格式的模型可以在支持 ONNX 的推理引擎中进行部署和推理。\n2. TensorFlow SavedModel：TensorFlow SavedModel 是 TensorFlow 框架的标准模型保存格式。它包含了模型的网络结构和参数，可以方便地在 TensorFlow 的推理环境中加载和使用。\n3. PyTorch JIT（Just-In-Time）：PyTorch JIT 是 PyTorch 的即时编译器，可以将 PyTorch 模型导出为优化的 Torch 脚本或 Torch 脚本模型。这种格式可以在没有 PyTorch 环境的情况下进行推理，并且具有更高的性能。\n4. Caffe Model：Caffe 是一个流行的深度学习框架，它使用自己的模型表示格式。导出为 Caffe 模型的文件可以在 Caffe 框架中进行部署和推理。\n5. TFLite（TensorFlow Lite）：TFLite 是 TensorFlow 的移动和嵌入式设备推理框架，支持在资源受限的设备上进行高效推理。模型可以导出为 TFLite 格式，以便在移动设备或嵌入式系统中进行部署。\n6. Core ML（Core Machine Learning）：Core ML 是苹果的机器学习框架，用于在 iOS 和 macOS 上进行推理。模型可以导出为 Core ML 格式，以便在苹果设备上进行部署。\n这些格式都提供了不同的优势和适用场景。选择合适的导出格式应该考虑到目标平台和部署环境的要求，以及所使用的深度学习框架的支持情况。\n模型输出的参数有如下\n参数名 类型 参数解释 0 format str 导出模型的格式 1 keras bool 表示是否使用Keras 2 optimize bool 用于在导出TorchScript模型时进行优化，以便在移动设备上获得更好的性能 3 int8 bool 用于在导出CoreML或TensorFlow模型时进行INT8量化 4 dynamic bool 用于在导出CoreML或TensorFlow模型时进行INT8量化 5 simplify bool 用于在导出ONNX模型时进行模型简化 6 opset int/optional 用于指定导出ONNX模型时的opset版本 7 workspace int 用于指定TensorRT模型的工作空间大小，以GB为单位 8 nms bool 用于在导出CoreML模型时添加非极大值抑制（NMS） 命令行命令如下: yolo task=detect mode=export model=best.pt format=onnx ",
    
    "date": "2024-11-25 12:00:00",
    "updated": "2024-11-25 12:00:00"
  }
  
  , 
  {
    "objectID": "1727265600",
    "permalink": "/post/article_2/",
    "title": "恒源云",
    
    "content": " 恒源云 为当涉及到深度学习的训练任务时，GPU的计算能力是不可或缺的。相对于传统的中央处理器（CPU），图形处理器（GPU）具有更强大的并行计算能力，能够显著加速深度学习模型的训练过程。深度学习算法通常涉及大量的矩阵运算和张量操作，而GPU的并行计算架构使得它们能够高效地执行这些计算，从而加速模型训练的速度。\n恒源云是一个经济高效的云计算平台，您可以通过恒源云的控制台或者命令行界面来管理实例、上传和下载数据、执行训练任务等。恒源云还提供了高度可定制的实例规格，您可以根据自己的需求选择适合的实例类型和配置，以最大程度地优化性能和成本。\n另一个恒源云的优势是其经济实惠的价格。相对于购买和维护专门的GPU设备，利用恒源云进行云端模型训练可以大大节省成本。恒源云提供了多种付费模式，包括按需付费和预付费套餐，使您能够根据自己的预算和需求进行灵活选择。\n上传数据集 在恒源云中我们需要通过终端来上传数据集文件，当在本地处理好了数据集文件以后，我们将其解压缩成zip文件的格式当然tar压缩包等格式的都可以。 这里推荐大家用OSS命令上传数据集,可以支持大规模的数据上传。\n在利用OSS进行上传之前我们需要下载一个文件，下载方式如下。\n完成之后，我们点击下载好的文件，会弹出命令行。\n在这里我们可以输入指令,我们先来输入version来检验下我们是否安装成功。\n当我们安装成功之后，我们先远程登录我们的账号和密码，输入Login\nlogin 当我们登录成功之后,我们就远程登录了我们的恒源云账号和密码,我们就可以在我们的账号下面建立存储我们数据的文件了。\n按照下图操作即可。\n当我们上传好一个文件之后,该文件就保存到我们的系统内了,我们可以随时在该终端页面下载该数据到我们后面步骤中创建的任何实例当中，利用如下命令\n(PS:最后一步需要我们经过下面的\u0026rsquo;利用云端训练YOLOv8模型\u0026rsquo;之后才可以进行，在我们创建完实例之后进行的操作步骤)\n利用云端训练YOLOv8模型 首先进入恒源云的官方网站\n恒源云官方网站\n然后进行注册和登录操作此步骤省略\n当我们注册和登录之后会进到控制台界面,然后点击创建实例进入到如下界面。\n在其中根据你的需求选择你的GPU型号,\n之后在同页面的最下面有一个实例镜像，可以在其中的下拉滚动条中选择你需要的PyTorch、TensorFlow或者其它框架的版本\n然后之后我们创建实例即可。\n首先开始时需要创建一会,然后才可以进行操作，等待一会创建成功后就会变成如下图的状态情况。\n我们按照图片的操作点击其中的\u0026quot;JupyterLab\u0026quot; 然后会弹出新的网页如下图。\n在其中hy-tmp是一个存放我们文件的文件夹,我们点击进去点击图片上的上传本地文件操作即可上传你的模型文件。终端就是一个输入命令的地方，我们点击终端命令，如下图所示。\n我们初始的时候是在系统的根目录下面,我们进行模型训练等操作进入hy-tmp目录也就是你上传文件的目录下面。\n我们利用cd 命令进入hy-tmp目录\ncd hy-tmp 进入其中以后，上传我们的文件。\n可以看到我把YOLOv8的官方下载的压缩包上传了进去，其为zip格式的压缩包。\n此时在命令行输入命令解压缩该文件\n输入unzip 文件名.zip解压文件\nunzip 文件名.zip cd到该文件目录\ncd 文件名 输入ll 看文件目录下的结构 ll 这里我们演示的是利用YOLOv8进行目标检测时候的训练流程进行演示,我们进入ultralytics\\cfg文件目录利用cd进入\ncd ultralytics\\cfg 同理我们输入ll看该文件下的目录结构\n可以看到其中有一个default.yaml文件,该文件就是我们进行训练模型的文件,我们可以在左侧的目录下看该文件的代码。 当然其中的配置,我就不在这里描述了,如果有需要可以看我的YOLOv8详细训练教程里面有具体的配置以及教程。当我们配置好了数据集以及选择的模型之后就可以在官方的模型基础上进行训练了de。 此时我们需要退到ultralytics-main的目录下面执行下面的文件就可以进行训练了。\nyolo task=detect mode=train model=datasets/yolo8n.yaml data=datasets/data.yaml epochs=100 batch=64 device=0 single_cls=True pretrained=yolov8n.pt PS：在我们的系统中python解释器已经默认帮我们配置好了,如果你想要执行一个py格式文件，我们只需要输入python 文件名.py文件即可\npython 文件名.py 到此本教程就结束,希望对你有所帮助。大家如有任何问题可以在评论区进行提问。 ",
    
    "date": "2024-09-25 12:00:00",
    "updated": "2024-09-25 12:00:00"
  }
  
  , 
  {
    "objectID": "1724068800",
    "permalink": "/post/%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3/",
    "title": "说明文档",
    
    "content": " 这篇文档主要介绍《基于YOLOv8的农田病虫害检测与分析》的代码实现部分，整篇论文的目的主要是改进YOLOv8的网络结构，使其在检测病虫害的精度和实时性上有所提升。接下来，我将介绍如何从零开始搭建起本项目。\n安装Python 到python的官方网站：https://www.python.org/下载，安装\n安装完成后，在命令行窗口运行：python，查看安装的结果，如下图：\n至此，Python安装完成，接下来还需要安装anaconda，这是一个python虚拟环境，特别适合管理python的环境。\n安装anaconda 到anaconda的官方网站：https://www.anaconda.com/download/success下载，并安装：\n安装成功后，会在开始菜单出现如下图所示：\nanaconda安装完成，接下来安装pycharm，主要用来编写代码。\n安装Pycharm 学生可以申请教育版\n支持，所有的软件安装完成。\nYOLOv8目录结构介绍 首先介绍整个项目的目录：\n和原来的YOLOv8相比，根目录新增一些训练的脚本和测试的脚本，比如train.py和Detect.py，当然也可以直接通过命令行的方式来实现，两者效果都是一样的。\n重点是ultralytics/nn目录，所有的改进模块都是在这里进行，在这里我新建了一个Addmodules的目录，里面是改进的各种模块，包括主干网络，颈部网络和检测头的改进。\n需要修改的部分我都已经作了修改，不用再做其他的改动\n还有一个重要的目录：ultralytics/cfg/models/Add，这里面放的都是yaml文件，其中改进的yaml文件都已经写好，不需要改动。\n以下是一个yaml文件的示例，其它的都是类似的结构，只是参数不同：\n安装项目的环境（非常重要） 环境配置非常重要，我当时配环境换了一周左右的时间，中间经历了各种报错，软件包不兼容的问题和显卡驱动匹配的问题，总之就是不好搞。为了方面复现工作，我已经把anaconda的环境导出为environment.yml，位于项目的根目录里面，创建虚拟环境的时候直接使用就可以\nanaconda虚拟环境 再anaconda prompt终端输入conda env create -f environment.yml，就可以根据environment.yml文件创建虚拟环境，创建好后，通过conda env list查看环境是否存在，如下图所示就表明创建成功：\n如果安装的时候出现torch相关的错误，大概率是你的显卡驱动和这里面的torch包版本不匹配，这个问题需要自行修改即可，网上关于这方面的资料很多。\n使用虚拟环境 虚拟环境创建完成之后，就可以在pycharm中使用，点击右下角，切换conda环境，选择刚才创建的虚拟环境。如果到了这一步还没有报错的话，恭喜你，已经完成了80%的工作。\n运行Detect.py脚本，测试检测效果，如果没有报错，接下来就是训练模型。\n训练脚本train.py 找到根目录的train.py文件，注释已经写的很清楚，如下图：\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) from ultralytics import YOLO if __name__ == \u0026#39;__main__\u0026#39;: model = YOLO(\u0026#39;yolov8-HSFPN.yaml\u0026#39;) # model.load(\u0026#39;yolov8n.pt\u0026#39;) # 是否加载预训练权重,科研不建议大家加载否则很难提升精度 model.train(data=r\u0026#39;D:/Downloads/YOLOv8/datasets/data.yaml\u0026#39;, # 如果大家任务是其它的\u0026#39;ultralytics/cfg/default.yaml\u0026#39;找到这里修改task可以改成detect, segment, classify, pose cache=False, imgsz=640, epochs=150, single_cls=False, # 是否是单类别检测 batch=4, close_mosaic=10, workers=0, device=\u0026#39;0\u0026#39;, optimizer=\u0026#39;SGD\u0026#39;, # using SGD # resume=\u0026#39;runs/train/exp21/weights/last.pt\u0026#39;, # 如过想续训就设置last.pt的地址 amp=True, # 如果出现训练损失为Nan可以关闭amp project=\u0026#39;runs/train\u0026#39;, name=\u0026#39;exp\u0026#39;, ) model = YOLO(\u0026lsquo;yolov8-HSFPN.yaml\u0026rsquo;)，把里面的yaml文件换成自己的yaml文件，我这里用的是yolov8-HSFPN.yaml，data=r\u0026rsquo;D:/Downloads/YOLOv8/datasets/data.yaml，同理，换成自己数据集的yaml文件，我这里的数据集是yolo格式。其它的参数可以按照自己的任务自行调整。\n还有一个检测的脚本，Detect.py:\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) from ultralytics import YOLO if __name__ == \u0026#39;__main__\u0026#39;: model = YOLO(\u0026#39;D:/Downloads/YOLOv8/result/result_8_HSFPN/train/exp/weights/best.pt\u0026#39;) # select your model.pt path model.predict(source=\u0026#39;D:/Downloads/YOLOv8/ultralytics/assets\u0026#39;, imgsz=640, project=\u0026#39;runs/detect\u0026#39;, name=\u0026#39;exp\u0026#39;, save=True, ) 同理，把best.pt换成你自己训练好的模型，source里面输入检测图片的路径，运行该脚本就可以开始检测，结果保存在runs/detect目录。\n开始训练 准备好数据集，最好是yolo格式的，我的数据集项目里自带了，不需要重新下载：\ndatasets目录里面就是我的数据集：有train，test，valid三个目录，分别存放训练集，测试集和验证集的图像和标签：\n准备这些之后，运行train.py文件，开始训练。如果报错的话，请自行上网查找，无非就是找不到数据集，某个包的版本不对，或者是GPU用不了，只能用CPU。\n训练结果 训练结果会保存在runs/train目录下，exp1,exp2,exp3的顺序，表示每一次的训练结果。\n上图就是训练完成后目录的结构，weights目录里面就是我们需要的模型：best.pts是效果最好的，最后也是需要这个，last.pt是最后一次的训练结果。\n总结 整个项目的改进工作我已经做好，复现的话只需装好对应的环境，修改train.py的参数，运行train.py就可以开始训练；修改Detect.py的参数，就可以检测。目前项目只针对检测任务，对于分割和分类没有做改进。\n经验之谈 （1）以下为两个重要库的版本，必须对应下载，否则会报错\npython == 3.9.7 pytorch == 1.12.1 timm == 0.9.12 # 此安装包必须要 mmcv-full == 1.6.2 # 不安装此包部分关于dyhead的代码运行不了以及Gold-YOLO\n（2）mmcv-full会安装失败是因为自身系统的编译工具有问题，也有可能是环境之间安装的有冲突\n推荐大家离线安装的形式,下面的地址中大家可以找找自己的版本,下载到本地进行安装。 https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html https://download.openmmlab.com/mmcv/dist/index.html （3）basicsr安装失败原因,通过pip install basicsr 下载如果失败,大家可以去百度搜一下如何换下载镜像源就可以修复\n针对一些报错的解决办法在这里说一下 (1)训练过程中loss出现Nan值. 可以尝试关闭AMP混合精度训练.\n(2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个\npython -m torch.distributed.run --nproc_per_node 2 train.py python -m torch.distributed.launch --nproc_per_node 2 train.py (3) 针对运行过程中的一些报错解决 1.如果训练的过程中验证报错了(主要是一些形状不匹配的错误这是因为验证集的一些特殊图片导致) 找到ultralytics/models/yolo/detect/train.py的DetectionTrainer class中的build_dataset函数中的rect=mode == \u0026lsquo;val\u0026rsquo;改为rect=False\n2.推理的时候运行detect.py文件报了形状不匹配的错误 找到ultralytics/engine/predictor.py找到函数def pre_transform(self, im),在LetterBox中的auto改为False 3.训练的过程中报错类型不匹配的问题 找到\u0026#39;ultralytics/engine/validator.py\u0026#39;文件找到 \u0026#39;class BaseValidator:\u0026#39; 然后在其\u0026#39;__call__\u0026#39;中 self.args.half = self.device.type != \u0026#39;cpu\u0026#39; # force FP16 val during training的一行代码下面加上self.args.half = False (4) 针对yaml文件中的nc修改 不用修改，模型会自动根据你数据集的配置文件获取。 这也是模型打印两次的区别，第一次打印出来的就是你选择模型的yaml文件结构，第二次打印的就是替换了你数据集的yaml文件，模型使用的是第二种。\n(5) 针对环境的问题 环境的问题每个人遇见的都不一样，可自行上网查找。\n",
    
    "date": "2024-08-19 12:00:00",
    "updated": "2024-08-19 12:00:00"
  }
  
  , 
  {
    "objectID": "1723377600",
    "permalink": "/post/article_11/",
    "title": "评估",
    
    "content": " 一、简介 这篇博客，主要给大家讲解我们在训练yolov8时生成的结果文件中各个图片及其中指标的含义，帮助大家更深入的理解，以及我们在评估模型时和发表论文时主要关注的参数有那些。本文通过举例训练过程中的某一时间的结果来帮助大家理解，大家阅读过程中如有任何问题可以在评论区提问出来，我会帮助大家解答。首先我们来看一个在一次训练完成之后都能生成多少个文件如下图所示，下面的文章讲解都会围绕这个结果文件来介绍。\n二、评估用的数据集 上面的训练结果，是根据一个检测飞机的数据集训练得来，其中只有个标签就是飞机，对于这种单标签的数据集，其实我们可以将其理解为一个二分类任务，\n一种情况-\u0026gt;检测为飞机，另一种情况-\u0026gt;不是飞机。\n三、结果分析 我们可以从结果文件中看到其中共有文件24个，后12张图片是根据我们训练过程中的一些检测结果图片，用于我们可以观察检测结果，有哪些被检测出来了，那些没有被检测出来，其不作为指标评估的文件。 Weights文件夹 我们先从第一个weights文件夹来分析，其中有两个文件，分别是best.pt、last.pt,其分别为训练过程中的损失最低的结果和模型训练的最后一次结果保存的模型。\nargs.yaml 第二个文件是args.yaml文件,其中主要保存一些我们训练时指定的参数，内容如下所示。\n混淆矩阵(ConfusionMatrix) 第三个文件就是混淆矩阵，大家都应该听过这个名字，其是一种用于评估分类模型性能的表格形式。它以实际类别（真实值）和模型预测类别为基础，将样本分类结果进行统计和汇总。\n对于二分类问题，混淆矩阵通常是一个2×2的矩阵，包括真阳性（True Positive, TP）、真阴性（True Negative, TN）、假阳性（False Positive, FP）和假阴性（False Negative, FN）四个元素。\nTrue_Label = [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1 ,0, 1, 0 , 1 , 0, 0 , 1]Predict_Label = [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1 ,0 , 0 , 1 , 0, 0 , 1, 0] 我们来分析这个图，其每个格子代表的含义我在图片上标注了出来**,下面我们来拿一个例子来帮助大家来理解这个混淆矩阵。**\n假设我们的数据集预测为飞机标记为数字0、预测不为飞机标记为1，现在假设我们在模型的训练的某一批次种预测了20次其真实结果和预测结果如下所示。 其中True_Label代表真实的标签，Predict_Label代表我们用模型预测的标签。\n那么我们可以进行对比产生如下分析\n6个样本的真实标签和预测标签都是0（真阴性，True Negative）。 1个样本的真实标签是0，但预测标签是1（假阳性，False Positive）。 8个样本的真实标签是1，但预测标签是0（假阴性，False Negative）。 5个样本的真实标签和预测标签都是1（真阳性，True Positive）。 下面根据我们的分析结果，我们就能够画出这个预测的混淆矩阵，\n由此我们就能得到那一批次的混淆矩阵，我们的最终结果生成的混淆矩阵可以理解为多个混淆矩阵的统计结果。 混淆矩阵归一化(Confusion Matrix Normal) 这个混淆矩阵的归一化，就是对混淆矩阵做了一个归一化处理，对混淆矩阵进行归一化可以将每个单元格的值除以该类别实际样本数，从而得到表示分类准确率的百分比。这种标准化使得我们可以直观地比较类别间的分类准确率，并识别出模型在哪些类别上表现较好或较差。\n我们可以看到是对于列进行了归一化处理，0.9 + 0.1 = 1，1 + 0 = 1。 计算mAP、Precision、Recall 在讲解其它的图片之前我们需要来计算三个比较重要的参数，这是其它图片的基础，这里的计算还是利用上面的某一批次举例的分析结果。\n精确度（Precision）：预测为正的样本中有多少是正确的，Precision = TP / (TP + FP) = 5 / (5 + 1) = 5/6 ≈ 0.833\n召回率（Recall）：真实为正的样本中有多少被正确预测为正，Recall = TP / (TP + FN) = 5 / (5 + 8) ≈ 0.385\nF1值（F1-Score）：**综合考虑精确度和召回率的指标，**F1 = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.833 * 0.385) / (0.833 + 0.385) ≈ 0.526\n准确度（Accuracy）：**所有样本中模型正确预测的比例，**Accuracy = (TP + TN) / (TP + TN + FP + FN) = (5 + 6) / (5 + 6 + 1 + 8) ≈ 0.565\n平均精确度（Average Precision, AP）：**用于计算不同类别的平均精确度，对于二分类问题，AP等于精确度。**AP = Precision = 0.833\n平均精确度（Mean Average Precision, mAP）：多类别问题的平均精确度，对于二分类问题，mAP等于AP（精确度），所以mAP = AP = 0.833\n这里需要讲解的主要是AP和MAP如果是多分类的问题，AP和mAP怎么计算，首先我们要知道AP的全称就是Average Precision，平均精度所以我们AP的计算公式如下？\nmAP就是Mean Average Precision，计算如下，计算每一个没别的AP进行求平均值处理就是mAP。\nF1_Curve F1_Curve这个文件，我们点击去的图片的标题是F1-Confidence Curve它显示了在不同分类阈值下的F1值变化情况。\n我们可以这么理解，先看它的横纵坐标，横坐标是置信度，纵坐标是F1-Score，F1-Score在前面我们以及讲解过了，那什么是置信度？\n**置信度(Confidence)-\u0026gt;**在我们模型的识别过程中会有一个概率，就是模型判定一个物体并不是百分百判定它是属于某一个分类，它会给予它以个概率，Confidence就是我们设置一个阈值，如果超过这个概率那么就确定为某一分类，假如我模型判定一个物体由0.7的概率属于飞机，此时我们设置的阈值如果为0.7以下那么模型就会输出该物体为飞机，如果我们设置的阈值大于0.7那么模型就不会输出该物体为飞机。\nF1-Confidence Curve就是随着F1-Score随着Confience的逐渐增高而变化的一个曲线。\nLabels Labels图片代表每个检测到的目标的类别和边界框信息。每个目标都由一个矩形边界框和一个类别标签表示，我们逆时针来看这个图片！！！\n目标类别：该像素点所检测到的目标类别，例如飞机等。 目标位置：该像素点所检测到的目标在图像中的位置，即该像素点在图像中的坐标。 目标大小：该像素点所检测到的目标的大小，即该像素点所覆盖的区域的大小。 其他信息：例如目标的旋转角度等其他相关信息。 labels_correlogram labels_correlogram是一个在**机器学习领域中使用的术语，**它指的是一种图形，用于显示目标检测算法在训练过程中预测标签之间的相关性。\n具体来说，labels_correlogram是一张颜色矩阵图，它展示了训练集数据标签之间的相关性。它可以帮助我们理解目标检测算法在训练过程中的行为和表现，以及预测标签之间的相互影响。\n通过观察labels_correlogram，我们可以了解到目标检测算法在不同类别之间的区分能力，以及对于不同类别的预测精度。此外，我们还可以通过比较不同算法或不同数据集labels_correlogram，来评估算法的性能和数据集的质量。\n总之，labels_correlogram是一种有用的工具，可以帮助我们更好地理解目标检测算法在训练过程中的行为和表现，以及评估算法的性能和数据集的质量。\nP_curve 这个图的分析和F1_Curve一样，不同的是关于的是Precision和Confidence之间的关系，可以看出我们随着置信度的越来越高检测的准确率按理来说是越来越高的。 R_curve 这个图的分析和F1_Curve一样，不同的是关于的是Recall和Confidence之间的关系，可以看出我们随着置信度的越来越高召回率的准确率按理来说是越来越低的。 PR_curve 它显示了在不同分类阈值下模型的精确度（Precision）和召回率（Recall）之间的关系。\nPR曲线越靠近坐标轴的右上角，模型性能越好，越能够正确识别正样本，正确分类正样本的Precision值越高，而靠近右侧则说明模型对正样本的识别能力较差，即召回能力较差。\nPR曲线的特点是随着分类阈值的变化，精确度和召回率会有相应的改变。通常情况下，当分类模型能够同时保持较高的精确度和较高的召回率时，PR曲线处于较高的位置。当模型偏向于高精确度或高召回率时，曲线则相应地向低精确度或低召回率的方向移动。\nPR曲线可以帮助我们评估模型在不同阈值下的性能，并选择适当的阈值来平衡精确度和召回率。对于模型比较或选择，我们可以通过比较PR曲线下方的面积（称为平均精确度均值，Average Precision, AP）来进行定量评估。AP值越大，模型的性能越好。\n总结：PR曲线是一种展示分类模型精确度和召回率之间关系的可视化工具，通过绘制精确度-召回率曲线，我们可以评估和比较模型在不同分类阈值下的性能，并计算平均精确度均值（AP）来定量衡量模型的好坏。\nresults.csv results.csv记录了一些我们训练过程中的参数信息，包括损失和学习率等，这里没有什么需要理解大家可以看一看，我们后面的results图片就是根据这个文件绘画出来的。\nresults 这个图片就是生成结果的最后一个了，我们可以看出其中标注了许多小的图片包括训练过程在的各种损失，我们主要看的其实就是后面的四幅图mAP50、mAP50-95、metrics/precision、metrics/recall四张图片。 mAP50：mAP是mean Average Precision的缩写，表示在多个类别上的平均精度。mAP50表示在50%的IoU阈值下的mAP值。 mAP50-95：这是一个更严格的评价指标，它计算了在50-95%的IoU阈值范围内的mAP值，然后取平均。这能够更准确地评估模型在不同IoU阈值下的性能。 metrics/precision：精度（Precision）是评估模型预测正确的正样本的比例。在目标检测中，如果模型预测的边界框与真实的边界框重合，则认为预测正确。 metrics/recall：召回率（Recall）是评估模型能够找出所有真实正样本的比例。在目标检测中，如果真实的边界框与预测的边界框重合，则认为该样本被正确召回。 检测效果图 最后的十四张图片就是检测效果图了，给大家看一下这里没什么好讲解的了。\n四、其它参数 FPS和IoU是目标检测领域中使用的两个重要指标，分别表示每秒处理的图片数量和交并比。\nFPS：全称为Frames Per Second，即每秒帧率。它用于评估模型在给定硬件上的处理速度，即每秒可以处理的图片数量。该指标对于实现实时检测非常重要，因为只有处理速度快，才能满足实时检测的需求。 IoU：全称为Intersection over Union，表示交并比。在目标检测中，它用于衡量模型生成的候选框与原标记框之间的重叠程度。IoU值越大，表示两个框之间的相似性越高。通常，当IoU值大于0.5时，认为可以检测到目标物体。这个指标常用于评估模型在特定数据集上的检测准确度。 在目标检测领域中，处理速度和准确度是两个重要的性能指标。在实际应用中，我们需要根据具体需求来平衡这两个指标。\n",
    
    "date": "2024-08-11 12:00:00",
    "updated": "2024-08-11 12:00:00"
  }
  
  , 
  {
    "objectID": "1722168000",
    "permalink": "/post/article_10/",
    "title": "可视化热力图",
    
    "content": " 一、本文介绍 本文给大家带来的机制是的可视化热力图功能，热力图)作为我们论文当中的必备一环，可以展示出我们呈现机制的有效性，本文的内容支持YOLOv8最新版本，同时支持视频讲解，本文的内容是根据检测头的输出内容，然后来绘图，产生6300张预测图片，从中选取出有效的热力图来绘图。\n在开始之前给大家推荐一下我的专栏，本专栏每周更新3-10篇最新前沿机制 | 包括二次创新全网无重复，以及融合改进(大家拿到之后添加另外一个改进机制在你的数据集上实现涨点即可撰写论文)，还有各种前沿顶会改进机制 |，更有包含我所有附赠的文件（文件内集成我所有的改进机制全部注册完毕可以直接运行）和交流群和视频讲解提供给大家。 二、项目完整代码 我们将这个代码，复制粘贴到我们YOLOv8的仓库里然后创建一个py文件存放进去即可。\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) warnings.simplefilter(\u0026#39;ignore\u0026#39;) import torch, yaml, cv2, os, shutil import numpy as np np.random.seed(0) import matplotlib.pyplot as plt from tqdm import trange from PIL import Image from ultralytics.nn.tasks import DetectionModel as Model from ultralytics.utils.torch_utils import intersect_dicts from ultralytics.utils.ops import xywh2xyxy from pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM from pytorch_grad_cam.utils.image import show_cam_on_image from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): # Resize and pad image while meeting stride-multiple constraints shape = im.shape[:2] # current shape [height, width] if isinstance(new_shape, int): new_shape = (new_shape, new_shape) # Scale ratio (new / old) r = min(new_shape[0] / shape[0], new_shape[1] / shape[1]) if not scaleup: # only scale down, do not scale up (for better val mAP) r = min(r, 1.0) # Compute padding ratio = r, r # width, height ratios new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r)) dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1] # wh padding if auto: # minimum rectangle dw, dh = np.mod(dw, stride), np.mod(dh, stride) # wh padding elif scaleFill: # stretch dw, dh = 0.0, 0.0 new_unpad = (new_shape[1], new_shape[0]) ratio = new_shape[1] / shape[1], new_shape[0] / shape[0] # width, height ratios dw /= 2 # divide padding into 2 sides dh /= 2 if shape[::-1] != new_unpad: # resize im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR) top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1)) left, right = int(round(dw - 0.1)), int(round(dw + 0.1)) im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color) # add border return im, ratio, (dw, dh) class yolov8_heatmap: def __init__(self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio): device = torch.device(device) ckpt = torch.load(weight) model_names = ckpt[\u0026#39;model\u0026#39;].names csd = ckpt[\u0026#39;model\u0026#39;].float().state_dict() # checkpoint state_dict as FP32 model = Model(cfg, ch=3, nc=len(model_names)).to(device) csd = intersect_dicts(csd, model.state_dict(), exclude=[\u0026#39;anchor\u0026#39;]) # intersect model.load_state_dict(csd, strict=False) # load model.eval() print(f\u0026#39;Transferred {len(csd)}/{len(model.state_dict())} items\u0026#39;) target_layers = [eval(layer)] method = eval(method) colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int32) self.__dict__.update(locals()) def post_process(self, result): logits_ = result[:, 4:] boxes_ = result[:, :4] sorted, indices = torch.sort(logits_.max(1)[0], descending=True) return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy() def draw_detections(self, box, color, name, img): xmin, ymin, xmax, ymax = list(map(int, list(box))) cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2) cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA) return img def __call__(self, img_path, save_path): # remove dir if exist if os.path.exists(save_path): shutil.rmtree(save_path) # make dir if not exist os.makedirs(save_path, exist_ok=True) # img process img = cv2.imread(img_path) img = letterbox(img)[0] img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = np.float32(img) / 255.0 tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device) # init ActivationsAndGradients grads = ActivationsAndGradients(self.model, self.target_layers, reshape_transform=None) # get ActivationsAndResult result = grads(tensor) activations = grads.activations[0].cpu().detach().numpy() # postprocess to yolo output post_result, pre_post_boxes, post_boxes = self.post_process(result[0]) for i in trange(int(post_result.size(0) * self.ratio)): if float(post_result[i].max()) \u0026lt; self.conf_threshold: break self.model.zero_grad() # get max probability for this prediction if self.backward_type == \u0026#39;class\u0026#39; or self.backward_type == \u0026#39;all\u0026#39;: score = post_result[i].max() score.backward(retain_graph=True) if self.backward_type == \u0026#39;box\u0026#39; or self.backward_type == \u0026#39;all\u0026#39;: for j in range(4): score = pre_post_boxes[i, j] score.backward(retain_graph=True) # process heatmap if self.backward_type == \u0026#39;class\u0026#39;: gradients = grads.gradients[0] elif self.backward_type == \u0026#39;box\u0026#39;: gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] else: gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] + grads.gradients[4] b, k, u, v = gradients.size() weights = self.method.get_cam_weights(self.method, None, None, None, activations, gradients.detach().numpy()) weights = weights.reshape((b, k, 1, 1)) saliency_map = np.sum(weights * activations, axis=1) saliency_map = np.squeeze(np.maximum(saliency_map, 0)) saliency_map = cv2.resize(saliency_map, (tensor.size(3), tensor.size(2))) saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max() if (saliency_map_max - saliency_map_min) == 0: continue saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min) # add heatmap and box to image cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=True) \u0026#34;不想在图片中绘画出边界框和置信度，注释下面的一行代码即可\u0026#34; cam_image = self.draw_detections(post_boxes[i], self.colors[int(post_result[i, :].argmax())], f\u0026#39;{self.model_names[int(post_result[i, :].argmax())]} {float(post_result[i].max()):.2f}\u0026#39;, cam_image) cam_image = Image.fromarray(cam_image) cam_image.save(f\u0026#39;{save_path}/{i}.png\u0026#39;) def get_params(): params = { \u0026#39;weight\u0026#39;: \u0026#39;yolov8n.pt\u0026#39;, # 训练出来的权重文件 \u0026#39;cfg\u0026#39;: \u0026#39;ultralytics/cfg/models/v8/yolov8n.yaml\u0026#39;, # 训练权重对应的yaml配置文件 \u0026#39;device\u0026#39;: \u0026#39;cuda:0\u0026#39;, \u0026#39;method\u0026#39;: \u0026#39;GradCAM\u0026#39;, # GradCAMPlusPlus, GradCAM, XGradCAM , 使用的热力图库文件不同的效果不一样可以多尝试 \u0026#39;layer\u0026#39;: \u0026#39;model.model[9]\u0026#39;, # 想要检测的对应层 \u0026#39;backward_type\u0026#39;: \u0026#39;all\u0026#39;, # class, box, all \u0026#39;conf_threshold\u0026#39;: 0.01, # 0.6 # 置信度阈值，有的时候你的进度条到一半就停止了就是因为没有高于此值的了 \u0026#39;ratio\u0026#39;: 0.02 # 0.02-0.1 } return params if __name__ == \u0026#39;__main__\u0026#39;: model = yolov8_heatmap(**get_params()) model(r\u0026#39;ultralytics/assets/bus.jpg\u0026#39;, \u0026#39;result\u0026#39;) # 第一个是检测的文件, 第二个是保存的路径 三、参数解析 下面上面项目核心代码的参数解析，共有7个，能够起到作用的参数并不多。 参数名 参数类型 参数讲解 0 weights str 用于检测视频的权重文件地址（可以是你训练好的，也可以是官方提供的） 1 cfg str 你选择的权重对应的yaml配置文件，请注意一定要对应否则会报错和不显示图片 2 device str 设备的选择可以用GPU也可以用CPU 3 method str 使用的热力图第三方库的版本，不同的版本效果也不一样。 4 layer str 想要检测的对应层，比如这里设置的是9那么检测的就是第九层 4 backward_type str 检测的类别 5 conf_threshold str 置信度阈值，有的时候你的进度条没有满就是因为没有大于这个阈值的图片了 6 ratio int YOLOv8一次产生6300张预测框，选择多少比例的图片绘画热力图。 四、项目的使用教程 4.1 步骤一 我们在Yolo仓库的目录下创建一个py文件将代码存放进去，如下图所示。\n4.2 步骤二 我们按照参数解析部分的介绍填好大家的参数，主要配置的有两个一个就是权重文件地址另一个就是图片的地址。\n4.3 步骤三 我们挺好之后运行文件即可，图片就会保存在同级目录下的新的文件夹result内。\n4.4 置信度和检测框 看下下面的说明就行。\n",
    
    "date": "2024-07-28 12:00:00",
    "updated": "2024-07-28 12:00:00"
  }
  
  , 
  {
    "objectID": "1713700800",
    "permalink": "/post/article_14/",
    "title": "报错",
    
    "content": " 一、本文介绍 本文为专栏内读者和我个人在训练YOLOv8时遇到的各种错误解决方案，你遇到的问题本文基本上都能够解决。\n二、 报错问题 # 以下为两个重要库的版本，大家可以对应下载，使用教程我会更新，时间还没来得及大家可以先看视频使用。\n项目环境：\npython == 3.9.7\npytorch == 1.12.1\ntimm == 0.9.12\nmmcv-full == 1.6.2\n(1)训练过程中loss出现Nan值. 可以尝试关闭AMP混合精度训练，如何关闭amp呢找到如下文件\u0026rsquo;ultralytics/cfg/default.yaml\u0026rsquo;，其中有一个参数是\n\\[True, False\\], True runs AMP check\n我们将其设置为False即可，默认时为True。\n.\n(2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个 python -m torch.distributed.run \u0026ndash;nproc_per_node 2 train.py\npython -m torch.distributed.launch \u0026ndash;nproc_per_node 2 train.py\n(3) 针对运行过程中的一些报错解决 1.如果训练的过程中验证报错了(主要是一些形状不匹配的错误这是因为验证集的一些特殊图片导致)\n就是有这种训练第一个epochs完成后开始验证的时候报错，下面的方法基本百分之九十都能够解决。\n找到ultralytics/models/yolo/detect/train.py的DetectionTrainer class中的build_dataset函数中的rect=mode == \u0026lsquo;val\u0026rsquo;改为rect=False\n2.推理的时候运行detect.py文件报了形状不匹配的错误\n找到ultralytics/engine/predictor.py找到函数def pre_transform(self, im),在LetterBox中的auto改为False\n3.训练的过程中报错类型不匹配的问题\n找到\u0026rsquo;ultralytics/engine/validator.py\u0026rsquo;文件找到 \u0026lsquo;class BaseValidator:\u0026rsquo; 然后在其\u0026rsquo;__call__\u0026lsquo;中\nself.args.half = self.device.type != \u0026lsquo;cpu\u0026rsquo; # force FP16 val during training的一行代码下面加上self.args.half = False\n(4) 针对yaml文件中的nc修改 不用修改，模型会自动根据你数据集的配置文件获取。\n这也是模型打印两次的区别，第一次打印出来的就是你选择模型的yaml文件结构，第二次打印的就是替换了你数据集的yaml文件，模型使用的是第二种。\n(5) 针对环境的问题 环境的问题我实在解决不过来，所以大家可以自行在网上搜索解决方案。\n(6) 训练过程中不打印GFLOpS 计算的GFLOPs计算异常不打印，所以需要额外修改一处， 我们找到如下文件\u0026rsquo;ultralytics/utils/torch_utils.py\u0026rsquo;文件内有如下的代码按照如下的图片进行修改，大家看好函数就行，其中红框的640可能和你的不一样， 然后用我给的代码替换掉整个代码即可。\ndef get_flops(model, imgsz=640): \u0026#34;\u0026#34;\u0026#34;Return a YOLO model\u0026#39;s FLOPs.\u0026#34;\u0026#34;\u0026#34; try: model = de_parallel(model) p = next(model.parameters()) # stride = max(int(model.stride.max()), 32) if hasattr(model, \u0026#39;stride\u0026#39;) else 32 # max stride stride = 640 im = torch.empty((1, 3, stride, stride), device=p.device) # input image in BCHW format flops = thop.profile(deepcopy(model), inputs=[im], verbose=False)[0] / 1E9 * 2 if thop else 0 # stride GFLOPs imgsz = imgsz if isinstance(imgsz, list) else [imgsz, imgsz] # expand if int/float return flops * imgsz[0] / stride * imgsz[1] / stride # 640x640 GFLOPs except Exception: return 0 (7) mmcv安装的解决方法 有的读者mmcv-full会安装失败是因为自身系统的编译工具有问题，也有可能是环境之间安装的有冲突 推荐大家离线安装的形式,下面的地址中大家可以找找自己的版本,下载到本地进行安装。 https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html https://download.openmmlab.com/mmcv/dist/index.html ",
    
    "date": "2024-04-21 12:00:00",
    "updated": "2024-04-21 12:00:00"
  }
  
  , 
  {
    "objectID": "1678903200",
    "permalink": "/post/hello/",
    "title": "Hello World",
    
    "content": "💘 博麗 霊夢 💘\n",
    
    "date": "2023-03-15 11:00:00",
    "updated": "2023-03-15 11:00:00"
  }
  
]